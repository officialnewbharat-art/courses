<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Science Specialization | Internadda Elite</title>
    
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=Playfair+Display:wght@700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11"></script>

    <style>
        :root {
            --primary: #4338ca; 
            --primary-hover: #3730a3;
            --dark-bg: #0f172a;
            --text-main: #1a202c;
            --text-light: #64748b;
            --white: #ffffff;
            --border: #e2e8f0;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Inter', sans-serif; background-color: #f8faff; color: var(--text-main); line-height: 1.6; }

        header { 
            background: var(--white); 
            border-bottom: 1px solid var(--border); 
            position: sticky; top: 0; z-index: 1000; padding: 15px 25px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }
        .nav-container { display: flex; align-items: center; justify-content: space-between; max-width: 1400px; margin: 0 auto; }
        .logo-box { display: flex; align-items: center; gap: 10px; text-decoration: none; }
        .logo-box img { height: 35px; border-radius: 6px; }
        .logo-text { font-weight: 800; font-size: 1.3rem; color: #1a202c; letter-spacing: -0.5px; }
        .logo-text span { color: var(--primary); }

        .sidebar {
            position: fixed; top: 0; left: -320px; width: 300px; height: 100vh;
            background: var(--dark-bg); z-index: 1100; padding: 30px 20px;
            transition: 0.4s ease; color: white;
            overflow-y: auto;
        }
        .sidebar.active { left: 0; }
        .sidebar-overlay { position: fixed; inset: 0; background: rgba(0,0,0,0.5); z-index: 1050; display: none; }
        .sidebar-overlay.active { display: block; }
        
        .module-item { 
            padding: 14px; border-radius: 10px; margin-bottom: 8px; cursor: pointer; 
            display: flex; align-items: center; gap: 12px; font-size: 0.9rem; transition: 0.2s;
        }
        .module-item.active { background: var(--primary); color: white; }
        .module-item.locked { opacity: 0.4; cursor: not-allowed; }

        .main-layout { padding: 40px 20px; display: flex; justify-content: center; }
        .content-card { 
            background: white; border-radius: 20px; padding: 60px; 
            max-width: 900px; width: 100%; border: 1px solid var(--border);
            box-shadow: 0 4px 20px rgba(0,0,0,0.03);
        }

        .module-tag { color: var(--primary); font-weight: 700; text-transform: uppercase; font-size: 0.85rem; letter-spacing: 1.5px; margin-bottom: 15px; display: block; }
        .mod-title { font-size: 2.2rem; font-weight: 800; color: var(--dark-bg); margin-bottom: 25px; }
        .section-head { font-size: 1.4rem; margin: 30px 0 15px; color: var(--primary); font-weight: 700; }
        .pro-note { background: #f0f4ff; border-left: 4px solid var(--primary); padding: 20px; border-radius: 8px; margin: 20px 0; font-style: italic; }
        pre { background: #1e293b; color: #e2e8f0; padding: 20px; border-radius: 10px; margin: 20px 0; overflow-x: auto; }

        .btn-primary, .btn-premium { 
            background: var(--primary); color: white; padding: 16px 35px; border-radius: 12px;
            border: none; font-weight: 700; cursor: pointer; transition: 0.3s;
            display: inline-flex; align-items: center; gap: 10px;
        }
        .btn-primary:hover, .btn-premium:hover { background: var(--primary-hover); transform: translateY(-2px); }

        .input-group { margin: 30px auto; text-align: left; max-width: 400px; }
        .input-premium { width: 100%; padding: 15px; border: 2px solid var(--border); border-radius: 10px; font-size: 1rem; }
        .hidden { display: none !important; }
    </style>
</head>
<body>

    <header>
        <div class="nav-container">
            <button style="background:none; border:none; cursor:pointer; font-size:1.4rem;" onclick="toggleSidebar()"><i class="fas fa-bars"></i></button>
            <a href="#" class="logo-box">
                <img src="https://internadda.com/images/logo.jpg" alt="Logo">
                <span class="logo-text">Intern<span>adda</span>
            </a>
            <div id="prog-stat" style="color:var(--primary); font-weight:700;">0% COMPLETE</div>
        </div>
    </header>

    <div class="sidebar-overlay" onclick="toggleSidebar()"></div>
    <aside class="sidebar" id="toc"></aside>

    <div id="welcome-screen" style="text-align: center; padding: 100px 20px;">
        <h1 class="hero-title">Advance Your Career with <br><span style="color:var(--primary)">Elite Data Science</span></h1>
        <p class="body-text" style="max-width:700px; margin: 20px auto 40px;">Master high-performance Python and industry-scale MLOps. Launch your career today with Internadda's proctored curriculum.</p>
        <button class="btn-primary" onclick="enrollUser()">Start Learning Now <i class="fas fa-arrow-right"></i></button>
    </div>

    <div id="course-ui" class="hidden">
        <main class="main-layout">
            <div id="main-content" class="content-card"></div>
        </main>
    </div>

    <div id="final-screen" class="hidden" style="text-align: center; padding: 100px 20px;">
        <h2 class="section-title">Claim Your Official Credential</h2>
        <p class="body-text">Congratulations on completing the Internadda Elite Specialization.</p>
        <div class="input-group">
            <input type="text" id="cert-name" class="input-premium" placeholder="Enter Full Name for Certificate">
        </div>
        <button class="btn-primary" onclick="generateCredential()">Generate Certificate <i class="fas fa-award"></i></button>
    </div>

    <script>
        let maxUnlocked = 0;
        let currentModule = 0;

        // Content synchronized with Internadda Elite Curriculum
const syllabus = [
    {
title: "Module 01: Professional Python Engineering",
content: `
    <span class="module-tag">System Architecture</span>
    <h2 class="mod-title">High-Performance Python & Memory Management</h2>

    <p class="body-text">
        This module transforms you from a Python coder into a <b>system-level Python engineer</b>.
        You will learn how Python actually runs behind the scenes, how memory is allocated,
        how performance is optimized, and how large-scale applications are designed in industry.
    </p>

    <p class="body-text">
        Real engineering does not mean writing long scripts.
        It means:
        <ul>
            <li>building predictable architectures</li>
            <li>reducing memory consumption</li>
            <li>writing scalable modules</li>
            <li>designing maintainable systems used by real users</li>
        </ul>
    </p>

    <!-- NEW SUBTOPIC 1 -->
    <h3 class="section-head">1. CPython Architecture & Execution Model</h3>

    <p class="body-text">
        Python is not “just interpreted”.
        Your code is first converted into <b>bytecode</b>, then executed by the <b>CPython Virtual Machine</b>.
        Understanding this flow helps you predict performance behavior.
    </p>

    <div class="pro-note">
        <b>Execution flow:</b>
        <ul>
            <li>Source Code (.py)</li>
            <li>Bytecode (.pyc)</li>
            <li>Interpreter executes bytecode</li>
            <li>Underlying C implementations run operations</li>
        </ul>
        This explains why Python is powerful — it sits on top of optimized C libraries.
    </div>

    <!-- UPDATED MEMORY TOPIC -->
    <h3 class="section-head">2. Memory Allocation, Reference Counting & Garbage Collection</h3>

    <p class="body-text">
        Python manages memory automatically using:
        <ul>
            <li>a private heap</li>
            <li>reference counters for each object</li>
            <li>a garbage collector for cyclic references</li>
        </ul>
        Understanding this prevents memory leaks and crashes in long-running applications.
    </p>

    <p class="body-text">
        The <b>Global Interpreter Lock (GIL)</b> ensures only one thread executes Python bytecode at a time.
        This affects design choices for concurrency and performance tuning.
    </p>

    <div class="pro-note">
        <b>Practical memory optimizations you will learn:</b>
        <ul>
            <li>__slots__ to reduce per-object RAM usage</li>
            <li>choosing tuple vs list vs array</li>
            <li>avoiding unnecessary object creation</li>
            <li>reusing objects using pools</li>
        </ul>
    </div>

    <!-- NEW DATA STRUCTURES TOPIC -->
    <h3 class="section-head">3. Choosing the Right Data Structure</h3>

    <p class="body-text">
        Wrong data structure = slow program.
        You will learn when to use:
        <ul>
            <li>list vs deque</li>
            <li>set vs list for lookups</li>
            <li>tuple vs list for immutability</li>
            <li>dict vs dataclass for structured data</li>
        </ul>
        Internal implementation is also covered (hash tables, resizing, collisions).
    </p>

    <!-- NEW CONCURRENCY TOPIC -->
    <h3 class="section-head">4. Concurrency Models in Real Systems</h3>

    <p class="body-text">
        You will master three professional concurrency styles:
    </p>

    <div class="pro-note">
        <ul>
            <li><b>Multithreading</b> – best for I/O bound programs</li>
            <li><b>Multiprocessing</b> – true parallel CPU execution</li>
            <li><b>Asyncio</b> – event loop based high-scaling network tasks</li>
        </ul>
        You will not just “learn syntax” — you will decide which model is right for which real-world scenario.
    </div>

    <!-- NEW TOPIC -->
    <h3 class="section-head">5. Design Patterns for Large Applications</h3>

    <p class="body-text">
        Industry software is built using patterns, not random code.
        In this module you implement:
        <ul>
            <li>Singleton for global services (DB, Logger)</li>
            <li>Factory for modular object creation</li>
            <li>Repository for clean database access</li>
            <li>Observer for event-driven systems</li>
        </ul>
        These patterns are used in Django, FastAPI, ML pipelines, and microservices.
    </p>

    <!-- NEW TOPIC -->
    <h3 class="section-head">6. Writing Python for Production, Not Just Practice</h3>

    <p class="body-text">
        You will follow professional engineering rules:
        <ul>
            <li>PEP-8 and PEP-20 guidelines</li>
            <li>type hints and static analysis</li>
            <li>modular project structure</li>
            <li>documentation and docstrings</li>
            <li>testable functions and unit testing</li>
        </ul>
    </p>

    <p class="body-text">
        By the end of this module you will not just know Python —
        you will be ready to work on <b>enterprise backend systems, scalable APIs, and data platforms</b>.
    </p>

    <button class="btn-premium" onclick="completeModule()">Unlock Module 02 <i class="fas fa-lock-open"></i></button>
`
},

{
    title: "Module 02: Advanced Data Wrangling",
    content: `
        <span class="module-tag">Data Engineering</span>
        <h2 class="mod-title">Petabyte-Scale Wrangling with Pandas 2.0 & PyArrow</h2>

        <p class="body-text">
            Data wrangling is where more than <b>80% of real-world data science time</b> is spent.
            In this module, you learn how to clean, combine, reshape and optimize datasets
            at scales ranging from Excel sheets to petabyte warehouses.
            We focus not on basic functions, but on <b>engineering-grade techniques</b>
            used in analytics companies, fintech, AI pipelines and data platforms.
        </p>

        <p class="body-text">
            You will move beyond CSV reading and learn how to:
            <ul>
                <li>work with columnar memory formats</li>
                <li>perform vectorized operations</li>
                <li>optimize joins and merges</li>
                <li>handle missing, dirty and duplicated data</li>
                <li>process time-series and streaming style datasets</li>
            </ul>
        </p>

        <!-- NEW SUBTOPIC -->
        <h3 class="section-head">1. Modern DataFrames: Pandas 2.0 + PyArrow Backend</h3>

        <p class="body-text">
            Pandas 2.0 introduced optional integration with <b>PyArrow as a backend</b>.
            Instead of storing data in Python objects, columns are stored in <b>Arrow columnar format</b>,
            greatly reducing memory usage and improving performance.
        </p>

        <div class="pro-note">
            <b>Benefits of Arrow-backed DataFrames:</b>
            <ul>
                <li>faster filtering and aggregation</li>
                <li>zero-copy data sharing between libraries</li>
                <li>GPU acceleration compatibility</li>
                <li>efficient compression and serialization</li>
            </ul>
            This is the same technology used inside big data systems like Spark, DuckDB and BigQuery.
        </div>

        <!-- ALREADY EXISTING TOPIC — EXPANDED -->
        <h3 class="section-head">2. The PyArrow Columnar Format</h3>

        <p class="body-text">
            Traditional row-based storage is slow for analytics because CPUs fetch unnecessary fields.
            <b>Columnar format</b> stores each column together, improving:
            <ul>
                <li>scan speed</li>
                <li>compression efficiency</li>
                <li>SIMD vector computation</li>
            </ul>
        </p>

        <p class="body-text">
            Arrow enables:
            <ul>
                <li>zero-copy conversion to NumPy, Pandas, Spark, TensorFlow</li>
                <li>cross-language support (Python, R, Java, C++)</li>
                <li>fast serialization for APIs & ML serving</li>
            </ul>
        </p>

        <!-- VECTORIZATION -->
        <h3 class="section-head">3. Vectorization: Eliminating Python Loops</h3>

        <p class="body-text">
            The biggest performance mistake is using Python loops on large datasets.
            <b>Vectorization</b> replaces loops with optimized C-level operations working on entire arrays at once.
            This allows millions of rows to be processed in milliseconds.
        </p>

        <pre>
# Senior Level Vectorization
df['log_income'] = np.log1p(df['income'])  # C-level execution, SIMD optimized

# Junior Level (Avoid this)
df['log_income'] = df['income'].apply(lambda x: math.log(x + 1))  # Python loop overhead
        </pre>

        <div class="pro-note">
            <b>Rule:</b> If you are using <code>for</code> or <code>apply()</code> on large data,
            you are leaving 90% performance on the table.
        </div>

        <!-- NEW CLEANING TOPIC -->
        <h3 class="section-head">4. Industrial-Grade Data Cleaning</h3>

        <p class="body-text">
            Real-world data is messy. You will learn how to handle:
            <ul>
                <li>missing values (MCAR, MAR, MNAR)</li>
                <li>outliers and extreme values</li>
                <li>string normalization</li>
                <li>duplicate detection and removal</li>
                <li>mixed data types inside the same column</li>
            </ul>
        </p>

        <p class="body-text">
            You will also learn when to <b>drop</b> vs <b>impute</b>
            using statistical and domain-driven choices.
        </p>

        <!-- TIME SERIES -->
        <h3 class="section-head">5. Time-Series Windowing & Resampling</h3>

        <p class="body-text">
            Financial markets, IoT sensor data, server logs and user analytics
            require time-aware operations. We will work with:
            <ul>
                <li>Rolling windows</li>
                <li>EWMA (Exponentially Weighted Moving Average)</li>
                <li>Resampling (upsampling & downsampling)</li>
                <li>Irregular timestamps and missing intervals</li>
            </ul>
        </p>

        <!-- JOINS & MERGES -->
        <h3 class="section-head">6. High-Performance Joins & Merges</h3>

        <p class="body-text">
            Joining tables incorrectly is the fastest way to crash memory.
            You will master:
            <ul>
                <li>inner, outer, left, right joins</li>
                <li>index-based joins vs key-based joins</li>
                <li>merge conflicts and deduplication</li>
                <li>join-order optimization</li>
            </ul>
        </p>

        <p class="body-text">
            You will also learn how databases and Spark optimize joins internally,
            preparing you for big-data engineering roles.
        </p>

        <button class="btn-premium" onclick="completeModule()">Unlock Module 03 <i class="fas fa-lock-open"></i></button>
    `
},


 {
    title: "Module 03: Inferential Statistics",
    content: `
        <span class="module-tag">Mathematical Foundations</span>
        <h2 class="mod-title">Statistical Rigor & Bayesian Inference</h2>

        <p class="body-text">
            Statistics is the language of uncertainty. Elite data scientists do not just compute averages —
            they quantify how confident they are in their results.
            In this module, you will learn how to move from <b>descriptive summaries</b> to
            <b>decision-making under uncertainty</b> using formal statistical methods.
        </p>

        <p class="body-text">
            You will learn how to:
            <ul>
                <li>estimate population parameters from samples</li>
                <li>construct confidence intervals</li>
                <li>perform hypothesis testing</li>
                <li>interpret p-values correctly (not blindly)</li>
                <li>apply Bayesian reasoning in real systems</li>
            </ul>
        </p>

        <!-- CLT -->
        <h3 class="section-head">1. The Central Limit Theorem (CLT) & Distribution Analysis</h3>

        <p class="body-text">
            The Central Limit Theorem explains why the normal distribution appears everywhere.
            It states that when you take repeated samples and compute their means,
            the distribution of those means tends to be <b>approximately normal</b> —
            even if the original data is skewed or non-normal.
        </p>

        <div class="pro-note">
            <b>Why CLT matters in real life:</b>
            <ul>
                <li>forms the basis of Z-tests and t-tests</li>
                <li>enables confidence interval construction</li>
                <li>supports error estimation in machine learning models</li>
                <li>explains why many natural processes look “Gaussian”</li>
            </ul>
        </div>

        <p class="body-text">
            You will also learn to compare:
            <ul>
                <li>normal vs binomial vs Poisson distributions</li>
                <li>skewness and kurtosis</li>
                <li>heavy-tailed distributions (common in finance)</li>
            </ul>
        </p>

        <!-- A/B Testing -->
        <h3 class="section-head">2. A/B Testing, Hypothesis Testing & P-Value Interpretation</h3>

        <p class="body-text">
            A/B testing is the scientific backbone of product experiments,
            marketing campaigns, UI design, and pricing strategies.
            You will learn how to formally test whether one variant performs better than another.
        </p>

        <p class="body-text">
            Core concepts covered:
            <ul>
                <li>null vs alternative hypothesis</li>
                <li>type I and type II errors</li>
                <li>statistical power and sample size</li>
                <li>minimum detectable effect (MDE)</li>
                <li>p-hacking and false discovery control</li>
            </ul>
        </p>

        <div class="pro-note">
            <b>Correct interpretation of p-values:</b><br>
            A p-value does NOT tell you the probability that your hypothesis is true.<br>
            It tells you how surprising your observed data would be if the null hypothesis were true.
        </div>

        <!-- Bayesian -->
        <h3 class="section-head">3. Bayesian vs Frequentist Probability</h3>

        <p class="body-text">
            There are two major schools of statistical thinking.
        </p>

        <div class="pro-note">
            <ul>
                <li><b>Frequentist:</b> probability = long-run frequency of events</li>
                <li><b>Bayesian:</b> probability = belief updated as evidence arrives</li>
            </ul>
        </div>

        <p class="body-text">
            Using <b>Bayes' Theorem</b>,
            you will learn how to update prior beliefs when new data is observed.
            This has direct application in:
            <ul>
                <li>spam filtering</li>
                <li>medical diagnosis</li>
                <li>fraud detection</li>
                <li>recommender systems</li>
            </ul>
        </p>

        <!-- NEW TOPIC -->
        <h3 class="section-head">4. Confidence Intervals & Real-World Reporting</h3>

        <p class="body-text">
            Professionals never report “the mean is 45”.
            They report:
            <b>the mean is 45 ± 2.1 with 95% confidence</b>.
            You will learn how to compute and interpret confidence intervals
            for means, proportions, and regression coefficients.
        </p>

        <p class="body-text">
            You will also understand the difference between:
            <ul>
                <li>confidence intervals</li>
                <li>prediction intervals</li>
                <li>credible intervals (Bayesian)</li>
            </ul>
        </p>

        <p class="body-text">
            By the end of this module you will think like a statistician —
            questioning claims, validating evidence, and measuring uncertainty rigorously.
        </p>

        <button class="btn-premium" onclick="completeModule()">Unlock Module 04 <i class="fas fa-lock-open"></i></button>
    `
},

 {
    title: "Module 04: Supervised Learning (Regression)",
    content: `
        <span class="module-tag">Predictive Modeling</span>
        <h2 class="mod-title">Linear Systems, Cost Functions & Regularization</h2>

        <p class="body-text">
            Regression is not about drawing a best-fit line. It is about modeling the
            quantitative relationship between features and a continuous target variable.
            In this module, we mathematically connect
            <b>optimization</b>, <b>geometry</b>, and <b>statistics</b> to build robust predictive systems.
        </p>

        <p class="body-text">
            You will learn how to:
            <ul>
                <li>formulate regression as a linear algebra problem</li>
                <li>minimize cost functions using calculus</li>
                <li>regularize models to avoid overfitting</li>
                <li>evaluate regression beyond simple R-squared</li>
            </ul>
        </p>

        <!-- OLS -->
        <h3 class="section-head">1. The Math of Ordinary Least Squares (OLS)</h3>

        <p class="body-text">
            Ordinary Least Squares estimates parameters by minimizing the squared error 
            between predicted and actual values. You will learn the 
            <b>normal equation</b> and how gradient descent reaches the same solution iteratively.
        </p>

        <div class="pro-note">
            <b>Geometric intuition:</b>
            Regression projects the target vector onto the subspace spanned
            by the feature columns. Multicollinearity makes this projection unstable,
            causing coefficients to explode and interpretability to collapse.
        </div>

        <p class="body-text">
            Topics covered:
            <ul>
                <li>matrix formulation of regression</li>
                <li>rank deficiency and singular matrices</li>
                <li>Variance Inflation Factor (VIF)</li>
                <li>feature scaling and normalization</li>
            </ul>
        </p>

        <!-- Bias Variance -->
        <h3 class="section-head">2. Bias–Variance Tradeoff & Overfitting</h3>

        <p class="body-text">
            A good model does not just fit the training data — it generalizes.
            The bias–variance tradeoff explains why overly complex models overfit,
            while oversimplified ones underfit. We mathematically analyze how error components split into:
            <b>bias² + variance + irreducible noise</b>.
        </p>

        <div class="pro-note">
            <b>Regularization in practice:</b>
            <ul>
                <li><b>L1 (Lasso):</b> drives some coefficients exactly to zero → feature selection</li>
                <li><b>L2 (Ridge):</b> shrinks coefficients smoothly → stabilizes training</li>
                <li><b>Elastic Net:</b> hybrid of both for real-world robustness</li>
            </ul>
        </div>

        <p class="body-text">
            You will implement and compare:
            <ul>
                <li>Batch vs Stochastic Gradient Descent</li>
                <li>Learning rate scheduling</li>
                <li>Early stopping to prevent overfitting</li>
            </ul>
        </p>

        <!-- Metrics -->
        <h3 class="section-head">3. Advanced Regression Evaluation Metrics</h3>

        <p class="body-text">
            R-squared alone can be misleading. Professional modeling requires deeper diagnostics.
            We expand into:
        </p>

        <p class="body-text">
            <ul>
                <li><b>Adjusted R²</b> — penalizes unnecessary features</li>
                <li><b>MSE / RMSE / MAE</b> — error-based metrics</li>
                <li><b>AIC & BIC</b> — information-theoretic model selection</li>
                <li><b>MAPE</b> — business-friendly percentage error</li>
            </ul>
        </p>

        <div class="pro-note">
            <b>Residual analysis:</b>
            We visually test assumptions such as:
            <ul>
                <li>linearity</li>
                <li>homoscedasticity (equal variance)</li>
                <li>independence of errors</li>
                <li>normality of residuals</li>
            </ul>
            Real-world regression is invalid if these assumptions are broken.
        </div>

        <!-- NEW SECTION -->
        <h3 class="section-head">4. Polynomial & Regularized Regression in Production</h3>

        <p class="body-text">
            Real systems are rarely perfectly linear.
            We extend linear regression into <b>Polynomial Regression</b>
            and show how to control exploding coefficients with <b>regularization</b>.
        </p>

        <p class="body-text">
            Practical applications covered:
            <ul>
                <li>price prediction</li>
                <li>demand forecasting</li>
                <li>risk modeling</li>
                <li>healthcare progression modeling</li>
            </ul>
        </p>

        <p class="body-text">
            By the end of this module, you will be able to design,
            train, debug, and validate regression models that behave reliably in production environments.
        </p>

        <button class="btn-premium" onclick="completeModule()">Unlock Module 05 <i class="fas fa-lock-open"></i></button>
    `
},

 {
        title: "Module 05: Gradient Boosting Machines",
        content: `
            <span class="module-tag">SOTA Algorithms</span>
            <h2 class="mod-title">XGBoost, LightGBM, and Hyperparameter Optimization</h2>
            <p class="body-text">Gradient Boosting is the gold standard for tabular datasets in industry. From credit scoring to fraud detection and recommendation engines — GBMs consistently outperform deep learning on structured data.</p>
            
            <h3 class="section-head">1. The Boosting Mechanism</h3>
            <p class="body-text">
                Boosting builds models sequentially. Every next tree focuses on the errors of the previous tree.<br>
                We deeply understand:<br>
                • Residual learning<br>
                • Additive model building<br>
                • Gradient Descent in function space<br><br>
                You will see how the model learns:<br>
                <b>Prediction = Σ (weak learners × learning rate)</b>
            </p>

            <h3 class="section-head">2. Why Boosting Beats Bagging</h3>
            <p class="body-text">
                Random Forests reduce variance using bagging.<br>
                Boosting reduces both <b>bias + variance</b> together.<br><br>
                We compare:
                • Random Forest vs Gradient Boosting<br>
                • Independent vs sequential trees<br>
                • Majority vote vs weighted sum<br>
            </p>

            <h3 class="section-head">3. Handling Imbalanced & Noisy Data</h3>
            <p class="body-text">
                Real-world data is never clean. You learn:<br>
                • scale_pos_weight tuning<br>
                • class-weight approaches<br>
                • label smoothing<br>
                • robust loss functions (Huber, Quantile)<br><br>
                Use cases:<br>
                • fraud detection<br>
                • medical diagnosis<br>
                • churn prediction<br>
            </p>

            <h3 class="section-head">4. Feature Importance & SHAP Values</h3>
            <p class="body-text">
                We don’t just build models — we explain them.<br>
                You will understand:<br>
                • Global feature importance<br>
                • Local (per-prediction) explanations<br>
                • SHAP summary plots & dependence plots<br><br>
                This is critical for:<br>
                • Finance & banking regulations<br>
                • Healthcare models<br>
                • Any high-stakes AI system<br>
            </p>

            <h3 class="section-head">5. Regularization & Overfitting Control</h3>
            <p class="body-text">
                Boosting can easily overfit if uncontrolled.<br>
                We master:<br>
                • learning_rate<br>
                • max_depth<br>
                • min_child_weight<br>
                • gamma<br>
                • subsample & colsample_bytree<br><br>
                You learn how to create models that<br>
                <b>generalize well in the real world</b>.
            </p>

            <h3 class="section-head">6. Automated Hyperparameter Tuning with Optuna</h3>
            <p class="body-text">
                You will not tune models manually — you automate.<br>
                We implement:<br>
                • Bayesian optimization<br>
                • Pruning bad trials<br>
                • Searching high-dimensional spaces<br><br>
                Objective functions minimize:<br>
                • Logloss<br>
                • RMSE<br>
                • AUC-ROC<br>
            </p>

            <h3 class="section-head">7. Deployment & Production Considerations</h3>
            <p class="body-text">
                ML is useless unless deployed.<br>
                We cover:<br>
                • model compression<br>
                • inference latency optimization<br>
                • handling concept drift<br>
                • retraining pipelines<br><br>
                You will know how GBMs work in:<br>
                • fintech apps<br>
                • recommendation systems<br>
                • real-time scoring APIs<br>
            </p>

            <button class="btn-premium" onclick="completeModule()">Unlock Module 06 <i class="fas fa-lock-open"></i></button>
        `
},
{
        title: "Module 06: Dimensionality Reduction",
        content: `
            <span class="module-tag">Unsupervised Learning</span>
            <h2 class="mod-title">PCA, t-SNE, UMAP, and Manifold Learning</h2>
            <p class="body-text">
                Real-world datasets often contain hundreds or thousands of features. This leads to the <b>Curse of Dimensionality</b>: models overfit, distance metrics break down, and visualization becomes impossible. Dimensionality Reduction converts high-dimensional data into compact, meaningful representations without losing important structure.
            </p>

            <h3 class="section-head">1. Principal Component Analysis (PCA)</h3>
            <p class="body-text">
                PCA is the workhorse of dimensionality reduction. It is a linear transformation technique that rotates the feature space to new axes called <b>Principal Components</b>.<br><br>
                You will learn:
                • Eigenvalues & Eigenvectors<br>
                • Covariance matrix construction<br>
                • Scree plots & Explained Variance Ratio<br>
                • Whitening transformations<br><br>
                Applications:
                • noise reduction<br>
                • feature compression before ML<br>
                • visualization of 100D data in 2D<br>
                • speeding up training time dramatically
            </p>

            <div class="pro-note">
                <b>Pro Insight:</b> PCA is not just compression — it reveals hidden structure by aligning axes in the direction of maximum variance. In finance, PCA is used to extract risk factors from correlated assets.
            </div>

            <h3 class="section-head">2. Nonlinear Embedding: t-SNE & UMAP</h3>
            <p class="body-text">
                PCA fails when the underlying data lies on a nonlinear manifold. For such complex geometry we use:<br>
                • <b>t-SNE (t-Distributed Stochastic Neighbor Embedding)</b><br>
                • <b>UMAP (Uniform Manifold Approximation and Projection)</b><br><br>
                These techniques preserve <b>local neighborhood structure</b> instead of global distances, allowing clusters to emerge visually.<br><br>
                Use cases:
                • visualizing word embeddings<br>
                • customer segmentation<br>
                • genomic data exploration<br>
                • image feature visualization from CNNs
            </p>

            <h3 class="section-head">3. When & Why to Reduce Dimensions</h3>
            <p class="body-text">
                Dimensionality reduction is not only for visualization — it directly improves model quality.<br><br>
                Benefits include:<br>
                • reduced overfitting<br>
                • faster training & inference<br>
                • removal of multicollinearity<br>
                • lower memory footprint<br><br>
                You also learn when <b>not</b> to reduce dimensions:
                • interpretability loss is critical<br>
                • when features are already sparse encoded<br>
                • when autoencoders are preferable
            </p>

            <h3 class="section-head">4. Linear vs Nonlinear Manifold Learning</h3>
            <p class="body-text">
                We compare:<br>
                • Linear methods (PCA, SVD)<br>
                • Nonlinear embeddings (Isomap, LLE, UMAP, t-SNE)<br><br>
                You will understand how data can actually live on a <b>lower-dimensional curved surface</b> embedded inside high-dimensional space — the core idea of manifold learning.
            </p>

            <button class="btn-premium" onclick="completeModule()">Unlock Module 07 <i class="fas fa-lock-open"></i></button>
        `
},


{
        title: "Module 07: Deep Learning & Neural Networks",
        content: `
            <span class="module-tag">Artificial Intelligence</span>
            <h2 class="mod-title">Backpropagation, Computational Graphs & Modern Architectures</h2>
            <p class="body-text">
                Deep Learning powers recommendation systems, ChatGPT, self-driving cars, and medical imaging. In this module, you move from using libraries to
                <b>understanding the mathematics that drives them</b>. We construct neural networks from scratch to truly understand gradients, weights, and loss functions.
            </p>

            <h3 class="section-head">1. Activation Functions & Vanishing Gradients</h3>
            <p class="body-text">
                Activation functions give neural networks the ability to learn nonlinear patterns. We compare:<br>
                • Sigmoid<br>
                • Tanh<br>
                • <b>ReLU</b> and Leaky-ReLU<br>
                • GELU and Swish (modern activations)<br><br>
                Core concepts you master:<br>
                • why Sigmoid saturates and causes vanishing gradients<br>
                • how ReLU solves this issue in deep networks<br>
                • dead ReLU problem and when to use Leaky-ReLU<br>
                • gradient flow across multiple hidden layers
            </p>

            <div class="pro-note">
                <b>Pro Insight:</b> Vanishing gradients destroy learning in very deep networks. Residual Networks (ResNets) introduced skip connections to allow gradients to flow backward efficiently.
            </div>

            <h3 class="section-head">2. Backpropagation & Computational Graphs</h3>
            <p class="body-text">
                Backpropagation is simply repeated application of the <b>Chain Rule</b> of calculus over a computational graph.<br><br>
                We break down:
                • forward pass (prediction)<br>
                • loss computation (error)<br>
                • backward pass (gradient propagation)<br>
                • weight updates using optimizers<br><br>
                You will implement a mini neural network with:<br>
                • manual gradients<br>
                • no frameworks<br>
                • full step-by-step matrix math<br><br>
                After this, TensorFlow and PyTorch become intuitive rather than magical.
            </p>

            <h3 class="section-head">3. Optimizer Architectures</h3>
            <p class="body-text">
                Training speed and stability depend on the optimizer. We compare:<br>
                • SGD (Stochastic Gradient Descent)<br>
                • Momentum<br>
                • RMSProp<br>
                • <b>Adam (Adaptive Moment Estimation)</b><br><br>
                You learn:<br>
                • exploding vs vanishing gradients<br>
                • why Adam works well for sparse gradients<br>
                • learning rate scheduling and warm restarts<br>
                • cosine annealing vs step decay
            </p>

            <h3 class="section-head">4. Modern Deep Network Architectures</h3>
            <p class="body-text">
                We introduce the architectures driving modern AI:<br>
                • Convolutional Neural Networks (CNNs) for images<br>
                • Recurrent Neural Networks (RNNs) & LSTMs for sequences<br>
                • Transformers & Attention mechanisms for language models<br><br>
                You will understand conceptually how ChatGPT-like models use:<br>
                • self-attention<br>
                • positional encoding<br>
                • multi-head attention blocks
            </p>

            <h3 class="section-head">5. Regularization & Generalization</h3>
            <p class="body-text">
                Deep networks overfit easily. We implement:<br>
                • Dropout<br>
                • Batch Normalization<br>
                • Data Augmentation<br><br>
                You learn how these techniques improve stability and reduce test-set error while keeping high accuracy.
            </p>

            <button class="btn-premium" onclick="completeModule()">Unlock Module 08 <i class="fas fa-lock-open"></i></button>
        `
},

 {
        title: "Module 08: Natural Language Processing (NLP)",
        content: `
            <span class="module-tag">Language Modeling</span>
            <h2 class="mod-title">The Transformer Revolution & Large Language Models (LLMs)</h2>
            <p class="body-text">
                NLP has moved from rule-based systems to deep neural architectures. Modern systems like ChatGPT,
                Gemini, and Claude are powered by <b>Transformers</b>, which learn contextual representations of language
                at massive scale.
            </p>

            <h3 class="section-head">1. Attention is All You Need</h3>
            <p class="body-text">
                Transformers replaced RNNs and LSTMs by removing sequential dependency in training.<br><br>
                You will understand:
                • Self-Attention mechanism<br>
                • Multi-Head Attention<br>
                • Query–Key–Value matrix operations<br>
                • Residual connections and Layer Normalization<br>
                • Positional Encoding (how models understand order without recurrence)<br><br>
                Key takeaway:
                Transformers process entire sequences <b>in parallel</b>, making them scalable for billion-parameter models.
            </p>

            <h3 class="section-head">2. Word Embeddings to Contextual Embeddings</h3>
            <p class="body-text">
                We transition from shallow NLP to deep representations:<br>
                • Bag-of-Words<br>
                • TF–IDF<br>
                • Word2Vec & GloVe (static embeddings)<br>
                • BERT-style contextual embeddings (meaning changes with sentence context)<br><br>
                You learn why “bank” means something different in:<br>
                • “river bank”<br>
                • “savings bank”
            </p>

            <h3 class="section-head">3. Fine-Tuning & Prompt Engineering</h3>
            <p class="body-text">
                Instead of training from scratch, enterprises fine-tune foundation models.<br><br>
                You will practice:
                • fine-tuning BERT, RoBERTa, and T5<br>
                • sentiment analysis, text classification, NER<br>
                • instruction-tuning and RLHF intuition<br><br>
                Prompt engineering concepts covered:<br>
                • zero-shot prompting<br>
                • few-shot prompting<br>
                • chain-of-thought prompting<br>
                • system prompts vs user prompts
            </p>

            <h3 class="section-head">4. Building Real-World NLP Applications</h3>
            <p class="body-text">
                We apply NLP to business use-cases:<br>
                • chatbot & virtual assistants<br>
                • resume parsing & job-matching systems<br>
                • document summarization<br>
                • spam detection & fraud messaging systems<br><br>
                You learn evaluation metrics:<br>
                • BLEU<br>
                • ROUGE<br>
                • perplexity
            </p>

            <h3 class="section-head">5. Ethics, Bias & Hallucination</h3>
            <p class="body-text">
                LLMs are powerful but imperfect. We study:<br>
                • dataset bias<br>
                • toxicity filtering<br>
                • hallucination in LLMs<br>
                • responsible AI development
            </p>

            <button class="btn-premium" onclick="completeModule()">Unlock Module 09 <i class="fas fa-lock-open"></i></button>
        `
},

{
        title: "Module 09: Computer Vision",
        content: `
            <span class="module-tag">Visual Perception</span>
            <h2 class="mod-title">Convolutional Neural Networks (CNNs) & Visual Intelligence</h2>
            <p class="body-text">
                Images are just matrices of pixel intensities — but AI learns to see <b>objects, faces, scenes, and emotions</b>.
                In this module, we move from raw pixels to high-level visual understanding using deep learning.
            </p>

            <h3 class="section-head">1. Kernels, Stride, and Padding</h3>
            <p class="body-text">
                Convolutional layers extract features automatically, replacing manual feature engineering.<br><br>
                You will learn:
                • convolution operation mathematics<br>
                • kernel/filter design<br>
                • stride & receptive field<br>
                • padding vs valid convolution<br>
                • max-pooling vs average-pooling<br><br>
                We visualize how filters detect:
                • edges<br>
                • corners<br>
                • textures<br>
                • high-level features (eyes, wheels, faces)
            </p>

            <h3 class="section-head">2. CNN Architecture Pipeline</h3>
            <p class="body-text">
                We build the full vision pipeline step-by-step:<br>
                • input image normalization<br>
                • feature extraction layers<br>
                • flattening & fully connected layers<br>
                • softmax output for classification<br><br>
                Datasets used:
                • MNIST<br>
                • CIFAR-10<br>
                • ImageNet (conceptually)
            </p>

            <h3 class="section-head">3. SOTA Architectures: ResNet, EfficientNet & MobileNet</h3>
            <p class="body-text">
                Real-world AI requires accuracy + computation efficiency.<br><br>
                You will understand:
                • Residual Connections (ResNet skip connections)<br>
                • depth vs width scaling (EfficientNet)<br>
                • lightweight models for mobile (MobileNet & depthwise separable convolutions)<br><br>
                Key outcome:
                You learn why deeper networks DON'T always mean better — 
                and how residual learning solved the vanishing gradient problem.
            </p>

            <h3 class="section-head">4. Object Detection & Segmentation</h3>
            <p class="body-text">
                Beyond classification, we detect and localize objects.<br><br>
                Methods covered:<br>
                • YOLO family (You Only Look Once)<br>
                • Faster R-CNN intuition<br>
                • Semantic vs Instance Segmentation<br><br>
                Real-world applications:
                • self-driving cars<br>
                • medical diagnosis<br>
                • CCTV monitoring<br>
                • retail analytics
            </p>

            <h3 class="section-head">5. Data Augmentation & Transfer Learning</h3>
            <p class="body-text">
                Training CNNs from scratch is expensive — so we reuse knowledge.<br><br>
                You will practice:<br>
                • image flipping, rotation, cropping<br>
                • color jitter & normalization<br>
                • using pre-trained models (ImageNet weights)<br>
                • freezing & fine-tuning layers<br><br>
                Result:
                Build high-accuracy models with <b>small datasets</b>.
            </p>

            <h3 class="section-head">6. Evaluating Vision Models</h3>
            <p class="body-text">
                We analyze performance using:<br>
                • confusion matrix<br>
                • precision & recall<br>
                • Intersection over Union (IoU)<br>
                • mAP (mean Average Precision)
            </p>

            <button class="btn-premium" onclick="completeModule()">Unlock Module 10 <i class="fas fa-lock-open"></i></button>
        `
},

{
    title: "Module 10: MLOps & Production Engineering",
    content: `
        <span class="module-tag">Deployment & ROI</span>
        <h2 class="mod-title">Model Serving, CI/CD, and Monitoring</h2>
        <p class="body-text">The final step is turning a model into a <b>Product</b>. This is what separates a hobby project from real-world AI systems generating business value.</p>
        
        <h3 class="section-head">1. API Development with FastAPI</h3>
        <p class="body-text">We build high-performance, asynchronous APIs to serve real-time predictions. You will design request/response schemas using <b>Pydantic</b>, implement batch prediction endpoints, and handle exceptions gracefully.</p>

        <h3 class="section-head">2. Containerization with Docker</h3>
        <p class="body-text">Package your complete ML environment — OS, Python version, dependencies, and model artifacts — into a <b>Docker Image</b>. Learn to work with Docker Hub, multi-stage builds, and GPU-enabled containers.</p>

        <h3 class="section-head">3. CI/CD for Machine Learning</h3>
        <p class="body-text">We implement automated pipelines using <b>GitHub Actions</b> and <b>GitLab CI</b>. Every code push triggers testing, linting, model validation, and deployment to staging or production environments.</p>

        <h3 class="section-head">4. Model Drift & Monitoring</h3>
        <p class="body-text">Models degrade as real-world data changes. You will track <b>Concept Drift</b>, <b>Data Drift</b>, latency, and failure rates using tools like Prometheus & Grafana — and trigger automated retraining when thresholds fail.</p>

        <h3 class="section-head">5. Scalable Deployment</h3>
        <p class="body-text">Serve millions of inferences using <b>Kubernetes</b>, <b>Ray Serve</b>, and serverless platforms. Understand auto-scaling, load balancing, blue–green deployment, and A/B testing of models in production.</p>

        <button class="btn-premium" onclick="completeModule()">Generate Final Credential <i class="fas fa-certificate"></i></button>
    `
}

];

        function toggleSidebar() {
            document.getElementById('toc').classList.toggle('active');
            document.querySelector('.sidebar-overlay').classList.toggle('active');
        }

        function enrollUser() {
            Swal.fire({
                title: 'Successfully Enrolled!',
                text: 'Welcome to Internadda Elite. Your proctored journey begins now.',
                icon: 'success',
                confirmButtonColor: '#4338ca'
            }).then(() => {
                document.getElementById('welcome-screen').classList.add('hidden');
                document.getElementById('course-ui').classList.remove('hidden');
                renderTOC();
                loadModule(0);
            });
        }

        function renderTOC() {
            const toc = document.getElementById('toc');
            let html = `<div style="text-transform:uppercase; font-size:0.7rem; color:rgba(255,255,255,0.6); margin-bottom:20px;">Curriculum</div>`;
            syllabus.forEach((mod, i) => {
                const isLocked = i > maxUnlocked;
                html += `
                    <div class="module-item ${i === currentModule ? 'active' : ''} ${isLocked ? 'locked' : ''}" onclick="${!isLocked ? `loadModule(${i})` : ''}">
                        <i class="fas ${i < maxUnlocked ? 'fa-check-circle' : (isLocked ? 'fa-lock' : 'fa-play-circle')}"></i>
                        ${mod.title}
                    </div>`;
            });
            toc.innerHTML = html;
        }

        function loadModule(idx) {
            currentModule = idx;
            document.getElementById('main-content').innerHTML = syllabus[idx].content;
            document.getElementById('prog-stat').innerText = `${Math.round(((maxUnlocked) / syllabus.length) * 100)}% COMPLETE`;
            renderTOC();
        }

        function completeModule() {
            if (currentModule < syllabus.length - 1) {
                maxUnlocked++;
                loadModule(currentModule + 1);
            } else {
                document.getElementById('course-ui').classList.add('hidden');
                document.getElementById('final-screen').classList.remove('hidden');
                document.getElementById('prog-stat').innerText = `100% COMPLETE`;
            }
        }

function generateCredential() {
    const name = document.getElementById('cert-name').value.trim();

    if (!name) {
        Swal.fire('Error', 'Please enter your full name.', 'error');
        return;
    }

    // Store name for certificate page
    localStorage.setItem("internadda_cert_name", name);

    Swal.fire({
        title: 'Certificate Generated!',
        text: `Official Internadda Elite Credential issued to ${name}.`,
        icon: 'success',
        confirmButtonText: 'View Certificate',
        confirmButtonColor: '#4338ca'
    }).then(() => {
        window.location.href = "certificate.html";
    });
}
    </script>
</body>
</html>


